// ============================================
// LLM Proxy Module Schema v2.0
// Single-Project, Multi-User Architecture
// Auto-creates users from JWT tokens
// ============================================

generator client {
  provider = "prisma-client-js"
}

datasource db {
  provider = "postgresql"
  url      = env("DATABASE_URL")
}

// ============================================
// Global App Configuration (Single Record)
// ============================================

model LLMProxyConfig {
  id              String   @id @default(uuid())
  appName         String   @default("AI Growth Pilot")
  
  // API Keys (stored in environment variables, not in DB)
  // This model just tracks which providers are enabled
  geminiEnabled   Boolean  @default(true)
  openaiEnabled   Boolean  @default(true)
  claudeEnabled   Boolean  @default(false)
  
  // Global Limits
  dailyTokenLimit     Int      @default(100000)
  monthlySpendLimit   Float    @default(100.0)
  
  // Features
  autoCreateUsers     Boolean  @default(true)
  enforceQuotas       Boolean  @default(true)
  trackUsage          Boolean  @default(true)
  
  createdAt       DateTime @default(now())
  updatedAt       DateTime @updatedAt
  
  @@map("llm_proxy_config")
}

// ============================================
// Users (Auto-created from JWT)
// ============================================

model LLMProxyUser {
  id              String   @id @default(uuid())
  
  // User Identity (from JWT)
  userId          String   @unique // From JWT 'sub' field
  email           String   @unique
  name            String?
  
  // Metadata
  createdFromJWT  Boolean  @default(true)
  firstSeenAt     DateTime @default(now())
  lastActiveAt    DateTime @default(now())
  
  // Status
  isActive        Boolean  @default(true)
  
  // Relations
  quota           LLMProxyUserQuota?
  usage           LLMProxyUsageLog[]
  
  createdAt       DateTime @default(now())
  updatedAt       DateTime @updatedAt
  
  @@index([userId])
  @@index([email])
  @@index([lastActiveAt])
  @@map("llm_proxy_users")

  cases Case[]
}

// ============================================
// Legal Cases (User Data)
// ============================================

model Case {
  id              String        @id @default(uuid())
  userId          String
  user            LLMProxyUser  @relation(fields: [userId], references: [id], onDelete: Cascade)
  
  title           String
  caseNumber      String?
  clientName      String
  opposingParty   String
  jurisdiction    String
  contextText     String        @db.Text
  
  createdAt       DateTime      @default(now())
  updatedAt       DateTime      @updatedAt

  @@index([userId])
  @@map("legal_cases")
}

// ============================================
// User Quotas (Per User)
// ============================================

model LLMProxyUserQuota {
  id                  String   @id @default(uuid())
  userId              String   @unique
  user                LLMProxyUser @relation(fields: [userId], references: [id], onDelete: Cascade)
  
  // Limits
  dailyTokenLimit     Int      @default(10000)
  monthlyTokenLimit   Int      @default(300000)
  requestsPerMinute   Int      @default(10)
  maxConcurrent       Int      @default(3)
  
  // Current Usage (auto-reset)
  currentDailyTokens  Int      @default(0)
  currentMonthlyTokens Int     @default(0)
  currentMinuteRequests Int    @default(0)
  currentConcurrent   Int      @default(0)
  
  // Reset Timestamps
  lastDailyReset      DateTime @default(now())
  lastMonthlyReset    DateTime @default(now())
  lastMinuteReset     DateTime @default(now())
  
  // Lifetime Stats
  totalTokensUsed     Int      @default(0)
  totalRequests       Int      @default(0)
  totalCost           Float    @default(0.0)
  
  createdAt           DateTime @default(now())
  updatedAt           DateTime @updatedAt
  
  @@index([userId])
  @@map("llm_proxy_user_quotas")
}

// ============================================
// Usage Logs (Per Request)
// ============================================

model LLMProxyUsageLog {
  id              String   @id @default(uuid())
  
  // User
  userId          String
  user            LLMProxyUser @relation(fields: [userId], references: [id], onDelete: Cascade)
  
  // LLM Call Details
  provider        String   // 'gemini' | 'openai' | 'claude'
  model           String   // e.g., 'gemini-2.0-flash-exp'
  operation       String   // 'generate' | 'generate-json' | 'chat'
  
  // Request Context
  endpoint        String   // e.g., '/api/llm-proxy/generate'
  method          String   @default("POST")
  
  // Usage Metrics
  promptTokens    Int      @default(0)
  completionTokens Int     @default(0)
  totalTokens     Int
  estimatedCost   Float    @default(0.0)
  
  // Performance
  latencyMs       Int      @default(0)
  
  // Status
  success         Boolean  @default(true)
  statusCode      Int      @default(200)
  errorMessage    String?
  
  // Client Info
  userAgent       String?
  ipAddress       String?
  
  // Timestamp
  timestamp       DateTime @default(now())
  
  @@index([userId, timestamp])
  @@index([provider, timestamp])
  @@index([success, timestamp])
  @@index([timestamp])
  @@map("llm_proxy_usage_logs")
}

// ============================================
// Bootstrap/Setup Status
// ============================================

model LLMProxyBootstrap {
  id                String   @id @default(uuid())
  
  // Setup Status
  isConfigured      Boolean  @default(false)
  hasApiKeys        Boolean  @default(false)
  
  // First Run
  firstBootstrapAt  DateTime @default(now())
  lastHealthCheckAt DateTime @default(now())
  
  // Stats
  totalUsers        Int      @default(0)
  totalRequests     Int      @default(0)
  
  version           String   @default("2.0.0")
  
  createdAt         DateTime @default(now())
  updatedAt         DateTime @updatedAt
  
  @@map("llm_proxy_bootstrap")
}
